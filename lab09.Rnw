\documentclass{article}
\usepackage{courier}
\usepackage{listings}
\usepackage{graphicx}

\title{Regression Analysis on the Relationship Between TV Advertising Budgets and
  Product Sales}
  
\author{Shannon Chang}

\date{October 6, 2016}
\begin{document}
\maketitle

\section{Abstract}
In this report, I will reproduce the scatterplot and fitted regression line shown in \textbf{Figure 3.1} 
(page 62) of \textit{An Introduction to Statistical Learning} by Gareth James, Daniela Witten, Trevor 
Hastie, and Robert Tibshirani. In addition, I will also reproduce  the summary regression coefficients 
shown in \textbf{Table 3.1} (page 68) and the quality indices shows in \textbf{Table 3.2} (page 69). These 
results are based on the \texttt{Advertising.csv} dataset that is paired with the textbook, and contains 
data on product sales in over two hundred different markets along with the advertising budgets for the 
product in each market by different mediums: \texttt{TV}, \texttt{Radio}, and \texttt{Newspaper}.

\section{Introduction}
Suppose a company wants advice on how to increase sales for one of its products. There is, of course, no 
concrete way to insure increased sales, but we can influence greater sales through advertising. Imagine 
that we are statistical consultants hired for this project. To convince the company to invest in 
advertising campaigns, we must first prove to the company that there is a relationship between advertising 
and sales. From here, we can then advise the company on appropriate advertising budgets to better reach 
sales targets. Thus, the goal for this analysis is to determine whether there is a relationship 
between advertising and sales and, if so, construct an accurate model that can be utilized to predict 
sales based on the size of advertising budget. For the purposes of this paper, we suppose that the company 
is only surveying TV advertising; therefore I will only construct analyses for TV advertising budgets 
and sales. This analyses can, however, be extended and compared to all remaining mediums in the dataset (radio and newspaper). 

\section{Data}
The \texttt{Advertising.csv} dataset consists of advertising budgets (in thousands of dollars) by medium: 
\texttt{TV}, \texttt{Radio}, and \texttt{Newspaper}. Product sales (in thousands of units) are listed 
under \texttt{Sales}. There are 200 rows of data, indicating 200 different markets. I will only be using data in the 
\texttt{TV} and \texttt{Sales} columns. Histograms for the two columns are shown below: 

\begin{figure}
  \includegraphics[width=\linewidth]{images/histogram-tv.png}
  \caption{Histogram of TV Advertising Budgets}
  \label{fig:tvhist}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{images/histogram-sales.png}
  \caption{Histogram of Product Sales}
  \label{fig:saleshist}
\end{figure}

\section{Methodology}
\subsection{Setting Up a Model}
To examine the association between \texttt{TV} and \texttt{Sales}, we model their relationship 
by \textit{simple linear regression}. This method involves predicting a quantitative 
response $Y$ based on the predictor variable $X$, assuming that there is a linear 
relationship between the two. For \texttt{TV} and \texttt{Sales}, we model their relationship as: 
$$Sales = \beta_0 + \beta_1TV$$
Here, $\beta_0$ represents the intercept of the linear model while $\beta_1$ 
represents the slope.
Since $\beta_0$ and $\beta_1$ are unknown, we would need to calculate estimates for 
the two coefficients instead. In a visual manner of speaking, we would want to 
graph all the data for `TV` and `Sales` and fit a line $Sales = \beta_0 + \beta_1TV$ 
as close as possible to our 200 data points. We can optimize the fit of this line 
using the \textit{least squares criterion}. This involves minimizing the sum of squared 
errors (distance between each data point and its predicted value from the linear 
model). The line/linear model that we fit would be based on an average of the 
squares.
From a computational perspective, we can start fitting the line by using sample 
means as estimates for $\beta_0$ and $\beta_1$, since the average of sample means 
over a large number of datasets will be very close to the actual/population mean. 
To evaluate the accuracy of these estimates, we start by calculating standard errors 
of the standard means. We can then use these standard errors to perform 
\textit{hypothesis tests} on the estimates. Thus, we would be testing the \textit{null hypothesis}
that $$H_0: There\: is\: no\: relationship\: between\: TV\:  and\:  Sales$$ versus the 
\textit{alternative hypothesis} that 
$$H_0: There\: is\: some\: relationship\: between\: TV\: and\: Sales$$
Numerically, we would be testing $$H_0: \beta_1 = 0$$ versus 
$$H_0: \beta_1 \neq 0$$
To do so, we would calculate a \textit{t-statistic}:
$$t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})}$$
This measures the number of standard deviations that our estimate for $\beta_1$ is 
away from 0.
From this, we can calculate a \textit{p-value}, which is the probability of observing 
any value greater than or equal to \textit{t}. A small p-value would indicate that it 
is unlikely to observe a meaningful association between the predictor (\texttt{TV}) and 
the response (\texttt{Sales}) purely by chance without some true relationship between 
the two. Thus, a small p-value would allow us to \textit{reject the null hypothesis} and 
determine that there is a relationship between \texttt{TV} and \texttt{Sales}. In general, 5\% 
or 1\% are used as p-value benchmarks.

\subsection{Evaluating Accuracy of the Model}
After conducting the hypothesis test, we will want to examine the extent to which 
the model fits the data. There are two quantities we can look at to assess this: 
\textit{residual standard error} and the $R^2$ statistic.

\subsection{Residual standard error (RSE)}
The RSE is an estimate of the standard deviation of errors, the distances from 
each data point to its predicted value based on the linear model we fit. In other 
words, it is the average amount that the response (\texttt{Sales}) will differ from 
the true regression line and is given by the formula: 
$$RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum{(y_i - \hat{y_i})^2}}$$ 

\subsection{$R^2$ statistic}
The $R^2$ statistic is, technically speaking, the proportion of variance explained 
by our fitted model. Specifically, it measures the \textit{proportion of variability in the 
response (\texttt{Sales}) that can be explained using the predictor (\texttt{TV})}. 
The closer $R^2$ is to 1, the greater the proportion of variability that is 
explained. Its formula is given by: 
$$R^2 = \frac{(TSS - RSS)}{TSS} = 1 - \frac{RSS}{TSS}$$
Here, the \textit{total sum of squares}, TSS = $\sum{(y_i - \bar{y})^2}$ measures the total 
variance in the response $Y$, and can be thought of as the amount of variability 
that already exists in the response, even before we perform any regression analysis. 
Thus, the $R^2$ value is a ratio of variability in $Y$ that can be explained by our 
model to the variability that exists inherently in $Y$.

\section{Results}
Using data collected in \textit{Advertising.csv} for TV advertising budgets and their 
corresponding product sales, I was able to generate the following calculations 
for the regression coefficients: 
<<chunkone, eval = TRUE, echo = FALSE, results = "asis", message = FALSE>>=
library(xtable) # load package
load("regression.RData") # load regression object

reg_coeffs <- xtable(regsum$coefficients)
print.xtable(reg_coeffs, comment = FALSE, caption.placement = "top")
@

A visualization of the coefficient estimates in relation to the observed data 
can be seen in this scatterplot: 
<<chunktwo, eval = TRUE, echo = FALSE, fig.width = 6, fig.height = 6, fig.align='center', fig.cap= "Predicting Product Sales Based on TV Advertising Budgets">>=
library(png) # load package
library(grid) # load package
img <- readPNG("images/scatterplot-tv-sales.png")
grid.raster(img)
@

Calculations for the quality indices yield the following:
<<chunkthree, eval = TRUE, echo = FALSE, results = "asis", message = FALSE>>=
library(xtable) # load package
load("regression.RData") # load regression object

rse <- regsum$sigma
r2 <- regsum$r.squared
f_stat <- as.numeric(regsum$fstatistic[1])

q_ind <- data.frame("Quantity" = c("Residual standard error", "R^2", "F-statistic"),
                    "Value" = round(c(rse, r2, f_stat), digits = 3))
q_tab <- xtable(q_ind,
                caption = c("Quality Indices for the Simple Linear Regression of Sales on TV"))
print.xtable(q_tab, comment = FALSE, caption.placement = "top", include.rownames = FALSE)
@

Note that the F-statistic is included here as well. This is usually utilized for 
multiple linear regression and thus, will not be discussed in this report. 

\section{Conclusions}
Since the p-value for the estimate of \texttt{TV} is essentially 0, we can reject the 
null hypothesis and infer that there is indeed a relationship between \texttt{TV} and 
\texttt{Sales}. From the quality indices, we can see that on average, the observed data 
deviates from its predicted value by \Sexpr{round(rse, digits = 2)}, meaning 
\$\Sexpr{1000*round(rse, digits = 2)}. The $R^2$ value tells 
us that \Sexpr{100*round(r2, digits = 2)}\% of the variability in \texttt{Sales} can 
be explained by \texttt{TV}. The fact that only about half the variability is explained 
by the model suggests that this simple linear regression may not be the best model fit for 
the relationship between \texttt{TV} and 
\texttt{Sales}.
\end{document}
